{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c88ce60-17b6-4031-8f19-2724a32c1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import os.path as path\n",
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873cd902-5c40-4fe0-bd24-c605de56b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5059ba-1824-4317-ab5b-b88f7cf27b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/09 03:18:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Divide CSV based on Column\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0f02810-8f10-4c63-9379-7ea563595302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitions_by_date(raw_folder_path, raw_csv_name, write_path):\n",
    "    raw_file_path = raw_folder_path + '/' + raw_csv_name\n",
    "    df = spark.read.option(\"header\", True).csv(raw_file_path)\n",
    "    df = df.withColumn(\"Year\", year(\"Date\"))\n",
    "    distinct_years = df.select(\"Year\").distinct().collect()\n",
    "\n",
    "    for row in distinct_years:\n",
    "        year_str\"Year\"])\n",
    "        filtered_df = df.filter(df[\"Year\"] == year_str        \n",
    "        # Define a dynamic path for each year\n",
    "        path = \"{}/{}_{}\".format(write_path, raw_csv_name, year_str)\n",
    "        \n",
    "        # Write the filtered DataFrame to a directory with CSV files inside it\n",
    "        filtered_df.write.csv(path, header=True, mode=\"overwrite\")\n",
    "        print(\"Wrote to {}\".format(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c88718-a4f9-41ab-8112-6bfa7fcfeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '/home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset'\n",
    "raw_data_list = os.listdir(raw_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085f351-d791-42fe-a185-ef64c1e838db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2018\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2015\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2013\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2014\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2019\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2020\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2012\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2016\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Facebook/Facebook.csv_2017\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2007\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2018\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2015\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2006\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2013\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2014\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2019\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2004\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2020\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2012\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2009\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2016\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2005\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2010\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2011\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2008\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Google/Google.csv_2017\n",
      "AnalysisException occurred: Unable to infer schema for CSV. It must be specified manually.Facebook_partitioned\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2003\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2007\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2018\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2015\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2006\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2013\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_1997\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2014\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2019\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2004\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_1998\n",
      "Wrote to /home/zhangr235_dev0/FANNG_stock_pipeline/raw_dataset/partitioned_Amazon/Amazon.csv_2020\n"
     ]
    }
   ],
   "source": [
    "for raw_data in raw_data_list:\n",
    "    write_path = raw_data_dir + '/partitioned_' + raw_data.split(\".\")[0]\n",
    "    try:\n",
    "        get_partitions_by_date(raw_data_dir, raw_data,  write_path)\n",
    "    except AnalysisException as e:\n",
    "    # Handle the AnalysisException\n",
    "        print(f\"AnalysisException occurred: {e.desc}\" + raw_data)\n",
    "    except Exception:\n",
    "        print(f\"An unexpected exception occurred: {str(e)}\"+ raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76775ebc-0dd7-4838-be44-e4a5c5ba9747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6eb32d-922e-45f7-9343-67adfeb0ff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488e402-9f12-42ad-81d2-38af58c3e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665a523-11ea-4f28-b89f-c14b3d7dfb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06861fd-d43a-48f5-ab3a-886ee60ab43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a4466-3f98-4014-a27d-a8068ab1081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f15388-5250-4f84-a8bb-224dcdce019d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
